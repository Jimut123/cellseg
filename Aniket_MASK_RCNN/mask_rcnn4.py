# -*- coding: utf-8 -*-
"""mask_rcnn4.ipynb

Aniket -  28/04/21

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18HPZ2z3evbhKcHHM_TRLz607hKyxLqFz
"""


# Commented out IPython magic to ensure Python compatibility

import warnings
warnings.filterwarnings("ignore")


import os
import sys
import random
import math
import numpy as np
import skimage.io
import matplotlib
import matplotlib.pyplot as plt
import cv2
from imgaug import augmenters as iaa
import pandas as pd

import tensorflow.compat.v1 as tf
# import tensorflow as tf

import keras
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)

tf.disable_v2_behavior()

device = '/gpu:0'

# %tensorflow_version 1.x
# Root directory of the project
ROOT_DIR = os.path.abspath("./Mask_RCNN/")
sys.path.append(ROOT_DIR)
# Import Mask RCNN
from mrcnn.config import Config
from mrcnn import utils
import mrcnn.model as modellib
from mrcnn import visualize
from mrcnn.model import log
from keras.callbacks import ModelCheckpoint
# Import COCO config
sys.path.append(os.path.join(ROOT_DIR, "samples/coco/"))  # find local version
import coco

# %matplotlib inline 

# Directory to save logs and trained model
MODEL_DIR = os.path.join(ROOT_DIR, "logs")

# Local path to trained weights file
COCO_MODEL_PATH = os.path.join(ROOT_DIR, "mask_rcnn_coco.h5")
# Download COCO trained weights from Releases if needed
if not os.path.exists(COCO_MODEL_PATH):
    utils.download_trained_weights(COCO_MODEL_PATH)

class BloodConfig(Config):
    """Configuration for training on the nucleus segmentation dataset."""
    # Give the configuration a recognizable name
    NAME = "blood"

    # Adjust depending on your GPU memory
    IMAGES_PER_GPU = 3

    # Number of classes (including background)
    NUM_CLASSES = 1 + 10  # Background + basophil et al

    # Number of training and validation steps per epoch
    STEPS_PER_EPOCH = 500
    VALIDATION_STEPS = 40

    # Don't exclude based on confidence. Since we have two classes
    # then 0.5 is the minimum anyway as it picks between basophil and BG
    DETECTION_MIN_CONFIDENCE = 0.7

    # Backbone network architecture
    # Supported values are: resnet50, resnet101
    BACKBONE = "resnet101"

    # # Input image resizing
    # # Random crops of size 64x64
    # IMAGE_RESIZE_MODE = "crop"
    # IMAGE_MIN_DIM = 64
    # IMAGE_MAX_DIM = 64
    # IMAGE_MIN_SCALE = 2.0

    # # Length of square anchor side in pixels
    # RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)

    # ROIs kept after non-maximum supression (training and inference)
    # POST_NMS_ROIS_TRAINING = 1000
    # POST_NMS_ROIS_INFERENCE = 2000

    # # Non-max suppression threshold to filter RPN proposals.
    # # You can increase this during training to generate more propsals.
    # RPN_NMS_THRESHOLD = 0.9

    # # How many anchors per image to use for RPN training
    # RPN_TRAIN_ANCHORS_PER_IMAGE = 64

    # # Image mean (RGB)
    # MEAN_PIXEL = np.array([43.53, 39.56, 48.22])


    # Number of ROIs per image to feed to classifier/mask heads
    # The Mask RCNN paper uses 512 but often the RPN doesn't generate
    # enough positive proposals to fill this and keep a positive:negative
    # ratio of 1:3. You can increase the number of proposals by adjusting
    # the RPN NMS threshold.
    TRAIN_ROIS_PER_IMAGE = 128

    # Maximum number of ground truth instances to use in one image
    MAX_GT_INSTANCES = 200

    # Max number of final detections per image
    DETECTION_MAX_INSTANCES = 100

class BloodInferenceConfig(BloodConfig):
    # Set batch size to 1 to run one image at a time
    GPU_COUNT = 1
    IMAGES_PER_GPU = 1
    # Don't resize imager for inferencing
    # IMAGE_RESIZE_MODE = "pad64"
    # Non-max suppression threshold to filter RPN proposals.
    # You can increase this during training to generate more propsals.
    RPN_NMS_THRESHOLD = 0.7

class_names = [
      "BAND CELLS","BASOPHILS", "BLAST CELLS", "EOSINOPHILS", "LYMPHOCYTES", "METAMYELOCYTES", "MONOCYTES", "MYELOCYTE", "NEUTROPHILS", "PROMYELOCYTES"
]

class_ids_dict = {
    "basophil": 1,
    "eosinophil": 2,
    "band cell": 3,
    "blast cell": 4,
    "lymphocyte": 5,
    "monocyte": 6,
    "neutrophil": 7,
    "promyelocyte": 8,
    "myelocyte": 9,
    "metamyelocyte": 10
}

from xml.dom import minidom
class BloodDataset(utils.Dataset):

    def load_blood(self, dataset_dir, subset):
        """Load a subset of the nuclei dataset.
        dataset_dir: Root directory of the dataset
        subset: Subset to load. Either the name of the sub-directory,
                such as stage1_train, stage1_test, ...etc. or, one of:
                * train: stage1_train excluding validation images
                * val: validation images from VAL_IMAGE_IDS
        """
        # Add classes. We have one class.
        # Naming the dataset nucleus, and the class nucleus
        self.add_class("blood", 1, "basophil")
        self.add_class("blood", 2, "eosinophil")
        self.add_class("blood", 3, "band cell")
        self.add_class("blood", 4, "blast cell")
        self.add_class("blood", 5, "lymphocyte")
        self.add_class("blood", 6, "monocyte")
        self.add_class("blood", 7, "neutrophil")
        self.add_class("blood", 8, "promyelocyte")
        self.add_class("blood", 9, "myelocyte")
        self.add_class("blood", 10, "metamyelocyte")
        # Which subset?

        assert subset in ["train", "val", "test"]

        for class_name in class_names:
          dataset_class_dir = os.path.join(dataset_dir, class_name, subset)
          image_dir = os.path.join(dataset_class_dir, "images")
          
          doc = minidom.parse(os.path.join(dataset_class_dir, "annotations.xml"))
          images = doc.getElementsByTagName('image')

          for image in images:
            image_name = image.attributes['name'].value
            width = image.attributes['width'].value
            height = image.attributes['height'].value

            polygons = []
            annotated_class_ids = []
            polygon_data = image.getElementsByTagName('polygon')
            for polygon in polygon_data:
              label = polygon.attributes['label'].value
              points = polygon.attributes['points'].value.split(';')
              all_points_x = []
              all_points_y = []
              for point in points:
                x = point.split(',')[0]
                y = point.split(',')[1]

                all_points_x.append(float(x))
                all_points_y.append(float(y))
              polygons.append({ "all_points_x": all_points_x, "all_points_y": all_points_y})
              annotated_class_ids.append(class_ids_dict[label])
              self.add_image(
                  "blood",
                  image_id = "{}/{}".format(class_name, image_name),
                  width=int(width),
                  height=int(height),
                  polygons=polygons,
                  path=os.path.join(dataset_class_dir, image_name),
                  class_ids=annotated_class_ids
              )

    def load_mask(self, image_id):
        """Generate instance masks for an image.
       Returns:
        masks: A bool array of shape [height, width, instance count] with
            one mask per instance.
        class_ids: a 1D array of class IDs of the instance masks.
        """
        image_info = self.image_info[image_id]
        if image_info["source"] != "blood":
            return super(self.__class__, self).load_mask(image_id)

        # Convert polygons to a bitmap mask of shape
        # [height, width, instance_count]
        info = self.image_info[image_id]
        mask = np.zeros([info["height"], info["width"], len(info["polygons"])],
                        dtype=np.uint8)
        for i, p in enumerate(info["polygons"]):
            # Get indexes of pixels inside the polygon and set them to 1
            rr, cc = skimage.draw.polygon(p['all_points_y'], p['all_points_x'])
            try:
                mask[rr, cc, i] = 1
            except IndexError:
                mask[rr-1,cc-1,i] = 1
        
        # Return mask, and array of class IDs of each instance. Since we have
        # one class ID, we return an array of ones
        return mask.astype(np.bool), np.array(info["class_ids"], dtype=np.uint8) # np.ones([mask.shape[-1]], dtype=np.int32)

    def image_reference(self, image_id):
        """Return the path of the image."""
        info = self.image_info[image_id]
        if info["source"] == "blood":
            return info["path"]
        else:
            super(self.__class__, self).image_reference(image_id)

def train(model, dataset_dir):
    """Train the model."""
    # Training dataset.
    dataset_train = BloodDataset()
    dataset_train.load_blood(dataset_dir, "train")
    dataset_train.prepare()

    # Validation dataset
    dataset_val = BloodDataset()
    dataset_val.load_blood(dataset_dir, "val")
    dataset_val.prepare()

    # Image augmentation
    # http://imgaug.readthedocs.io/en/latest/source/augmenters.html
    augmentation = iaa.SomeOf((0, 2), [
        iaa.Fliplr(0.5),
        iaa.Flipud(0.5),
        iaa.OneOf([iaa.Affine(rotate=90),
                   iaa.Affine(rotate=180),
                   iaa.Affine(rotate=270)]),
        iaa.Multiply((0.8, 1.5)),
        iaa.GaussianBlur(sigma=(0.0, 5.0))
    ])


    # If starting from imagenet, train heads only for a bit
    # since they have random weights
    print("Train network heads")
    check_path = os.path.join(MODEL_DIR, "mask_rcnn_{}_*epoch*.h5".format(
            "blood"))
    callbacks = [
    #     ModelCheckpoint(check_path,
    #                                         verbose=0, save_weights_only=False)
    ]
    history_network_heads = model.train(dataset_train, dataset_val,
                                learning_rate=config.LEARNING_RATE,
                                epochs=200,
                                augmentation=augmentation,
                                custom_callbacks=callbacks,
                                layers='heads')
    
    hist_df_nh = pd.DataFrame(history_network_heads.history)
    hist_json_file = 'history_network_heads.json'
    with open(hist_json_file, mode='w') as f:
        hist_df_nh.to_json(f)

    print("Train all layers")
    history_dataset_train = model.train(dataset_train, dataset_val,
                                learning_rate=config.LEARNING_RATE,
                                epochs=400,
                                augmentation=augmentation,
                                layers='all')
    
    # with open('all', 'w') as f:
    #     f.write(history_dataset_train)
    hist_df_dt = pd.DataFrame(history_dataset_train.history)
    hist_json_file = 'history_dataset_train.json'
    with open(hist_json_file, mode='w') as f:
        hist_df_dt.to_json(f)

config = BloodConfig()
model = modellib.MaskRCNN(mode="training", config=config,
                                  model_dir=MODEL_DIR)
# Start from ImageNet trained weights
weights_path = model.get_imagenet_weights()
model.load_weights(weights_path, by_name=True)

# with tf.device(device):
train(model, "./dataset")

def get_ax(rows=1, cols=1, size=8):
    """Return a Matplotlib Axes array to be used in
    all visualizations in the notebook. Provide a
    central point to control graph sizes.
    
    Change the default size attribute to control the size
    of rendered images
    """
    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))
    return ax

dataset = BloodDataset()
dataset.load_blood("./dataset", "test")
dataset.prepare()
# Load and display random samples


inference_config = BloodInferenceConfig()
model = modellib.MaskRCNN(mode="inference", config=inference_config,
                                  model_dir=MODEL_DIR)
weights_path = model.find_last()
model.load_weights(weights_path, by_name=True)

for image_id in dataset.image_ids:
    image_id = random.choice(dataset.image_ids)

    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\
        modellib.load_image_gt(dataset, inference_config, 
                            image_id, use_mini_mask=False)


    log("original_image", original_image)
    log("image_meta", image_meta)
    log("gt_class_id", gt_class_id)
    log("gt_bbox", gt_bbox)
    log("gt_mask", gt_mask)

    results = model.detect([original_image], verbose=1)

    def searchKeysByVal(dict, byVal):
        keysList = []
        itemsList = dict.items()
        for item in itemsList:
            if item[1] == byVal:
                keysList.append(item[0])
        return keysList

    r = results[0]
    label="True: {}\nPredictions:\n".format(dataset.image_reference(image_id).split("/")[-2])
    for c, s in zip(r['class_ids'], r['scores']):
        label += "{} ====> {}\n".format(searchKeysByVal(class_ids_dict, c),s)
    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], 
                            dataset.class_names, r['scores'], ax=get_ax(), title=label)
    plt.savefig("test_mask_{}".format(dataset.image_reference(image_id).replace("/", "_")))
    AP, precisions, recalls, overlaps = utils.compute_ap(gt_bbox, gt_class_id, gt_mask,
                                            r['rois'], r['class_ids'], r['scores'], r['masks'])
    visualize.plot_precision_recall(AP, precisions, recalls)
    plt.savefig("test_precision_recall_{}".format(dataset.image_reference(image_id).replace("/", "_")))
    visualize.plot_overlaps(gt_class_id, r['class_ids'], r['scores'],
                        overlaps, dataset.class_names)
    plt.savefig("test_overlaps_{}".format(dataset.image_reference(image_id).replace("/", "_")))


def compute_batch_ap(image_ids):
    APs = []
    for image_id in image_ids:
        # Load image
        image, image_meta, gt_class_id, gt_bbox, gt_mask =\
            modellib.load_image_gt(dataset, config,
                                   image_id, use_mini_mask=False)
        # Run object detection
        results = model.detect([image], verbose=0)
        # Compute AP
        r = results[0]
        AP, precisions, recalls, overlaps =\
            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,
                              r['rois'], r['class_ids'], r['scores'], r['masks'])
        APs.append(AP)
    return APs

# Pick a set of random images
# image_ids = np.random.choice(dataset.image_ids, 10)
APs = compute_batch_ap(dataset.image_ids)
print("mAP @ IoU=50: ", np.mean(APs))
