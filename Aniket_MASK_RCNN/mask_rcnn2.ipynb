{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mask_rcnn2",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SnMWC64P0Lo"
      },
      "source": [
        "%%shell\n",
        "rm -rf ./*\n",
        "gdown --id 1glwgR6XahSTV3t8qZS7qCO8WKVCryzhN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3CfidPVQPgE"
      },
      "source": [
        "%%shell\n",
        "unzip -qq basophil.zip\n",
        "mkdir dataset\n",
        "mv train dataset/\n",
        "mv val dataset/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxQsHrOPQYJM"
      },
      "source": [
        "!pip install keras==2.2.5\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B_JUUY2Qdwk"
      },
      "source": [
        "%%shell\n",
        "# clone Mask_RCNN repo and install packages\n",
        "git clone https://github.com/matterport/Mask_RCNN\n",
        "cd Mask_RCNN\n",
        "sed -i \"s/self.keras_model.metrics_tensors.append(loss)/self.keras_model.add_metric(loss, name)/g\" mrcnn/model.py\n",
        "python setup.py install > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSgTjzblQiX4"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import skimage.io\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from imgaug import augmenters as iaa\n",
        "%tensorflow_version 1.x\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"./Mask_RCNN/\")\n",
        "sys.path.append(ROOT_DIR)\n",
        "# Import Mask RCNN\n",
        "from mrcnn.config import Config\n",
        "from mrcnn import utils\n",
        "import mrcnn.model as modellib\n",
        "from mrcnn import visualize\n",
        "from mrcnn.model import log\n",
        "\n",
        "# Import COCO config\n",
        "sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # find local version\n",
        "import coco\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmkMmGYGrQtb"
      },
      "source": [
        "class BloodConfig(Config):\n",
        "    \"\"\"Configuration for training on the nucleus segmentation dataset.\"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"blood\"\n",
        "\n",
        "    # Adjust depending on your GPU memory\n",
        "    IMAGES_PER_GPU = 2\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 1  # Background + basophil\n",
        "\n",
        "    # Number of training and validation steps per epoch\n",
        "    STEPS_PER_EPOCH = 50\n",
        "    VALIDATION_STEPS = 10\n",
        "\n",
        "    # Don't exclude based on confidence. Since we have two classes\n",
        "    # then 0.5 is the minimum anyway as it picks between basophil and BG\n",
        "    DETECTION_MIN_CONFIDENCE = 0\n",
        "\n",
        "    # Backbone network architecture\n",
        "    # Supported values are: resnet50, resnet101\n",
        "    BACKBONE = \"resnet50\"\n",
        "\n",
        "    # # Input image resizing\n",
        "    # # Random crops of size 64x64\n",
        "    # IMAGE_RESIZE_MODE = \"crop\"\n",
        "    # IMAGE_MIN_DIM = 64\n",
        "    # IMAGE_MAX_DIM = 64\n",
        "    # IMAGE_MIN_SCALE = 2.0\n",
        "\n",
        "    # # Length of square anchor side in pixels\n",
        "    # RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
        "\n",
        "    # ROIs kept after non-maximum supression (training and inference)\n",
        "    # POST_NMS_ROIS_TRAINING = 1000\n",
        "    # POST_NMS_ROIS_INFERENCE = 2000\n",
        "\n",
        "    # # Non-max suppression threshold to filter RPN proposals.\n",
        "    # # You can increase this during training to generate more propsals.\n",
        "    # RPN_NMS_THRESHOLD = 0.9\n",
        "\n",
        "    # # How many anchors per image to use for RPN training\n",
        "    # RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
        "\n",
        "    # # Image mean (RGB)\n",
        "    # MEAN_PIXEL = np.array([43.53, 39.56, 48.22])\n",
        "\n",
        "\n",
        "    # Number of ROIs per image to feed to classifier/mask heads\n",
        "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
        "    # enough positive proposals to fill this and keep a positive:negative\n",
        "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
        "    # the RPN NMS threshold.\n",
        "    TRAIN_ROIS_PER_IMAGE = 30\n",
        "\n",
        "    # Maximum number of ground truth instances to use in one image\n",
        "    MAX_GT_INSTANCES = 200\n",
        "\n",
        "    # Max number of final detections per image\n",
        "    DETECTION_MAX_INSTANCES = 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft97aoGCsCDR"
      },
      "source": [
        "class BloodInferenceConfig(BloodConfig):\n",
        "    # Set batch size to 1 to run one image at a time\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    # Don't resize imager for inferencing\n",
        "    # IMAGE_RESIZE_MODE = \"pad64\"\n",
        "    # Non-max suppression threshold to filter RPN proposals.\n",
        "    # You can increase this during training to generate more propsals.\n",
        "    RPN_NMS_THRESHOLD = 0.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZjpoDqKsQpO"
      },
      "source": [
        "class BloodDataset(utils.Dataset):\n",
        "\n",
        "    def load_blood(self, dataset_dir, subset):\n",
        "        \"\"\"Load a subset of the nuclei dataset.\n",
        "        dataset_dir: Root directory of the dataset\n",
        "        subset: Subset to load. Either the name of the sub-directory,\n",
        "                such as stage1_train, stage1_test, ...etc. or, one of:\n",
        "                * train: stage1_train excluding validation images\n",
        "                * val: validation images from VAL_IMAGE_IDS\n",
        "        \"\"\"\n",
        "        # Add classes. We have one class.\n",
        "        # Naming the dataset nucleus, and the class nucleus\n",
        "        self.add_class(\"blood\", 1, \"basophil\")\n",
        "\n",
        "        # Which subset?\n",
        "        \n",
        "        assert subset in [\"train\", \"val\"]\n",
        "        dataset_dir = os.path.join(dataset_dir, subset)\n",
        "        image_dir = os.path.join(dataset_dir, \"images\")\n",
        "     \n",
        "        # Get image ids from directory names\n",
        "        image_ids = list(map(lambda x: x.split('.')[0], next(os.walk(image_dir))[2]))\n",
        "\n",
        "        # Add images\n",
        "        for image_id in image_ids:\n",
        "          print(image_id)\n",
        "          self.add_image(\n",
        "              \"blood\",\n",
        "              image_id=image_id,\n",
        "              path=os.path.join(dataset_dir, \"images/{}.jpg\".format(image_id)))\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        \"\"\"Generate instance masks for an image.\n",
        "       Returns:\n",
        "        masks: A bool array of shape [height, width, instance count] with\n",
        "            one mask per instance.\n",
        "        class_ids: a 1D array of class IDs of the instance masks.\n",
        "        \"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        # Get mask directory from image path\n",
        "        mask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), \"masks\")\n",
        "\n",
        "        # Read mask files from .png image\n",
        "        mask = []\n",
        "        mask_file_name = os.path.join(mask_dir, \"mask_{}.png\".format(info[\"id\"]))\n",
        "        m = skimage.io.imread(mask_file_name).astype(np.bool)\n",
        "        mask.append(m)\n",
        "        mask = np.stack(mask, axis=-1)\n",
        "        # Return mask, and array of class IDs of each instance. Since we have\n",
        "        # one class ID, we return an array of ones\n",
        "        return mask, np.ones([mask.shape[-1]], dtype=np.int32)\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        \"\"\"Return the path of the image.\"\"\"\n",
        "        info = self.image_info[image_id]\n",
        "        if info[\"source\"] == \"blood\":\n",
        "            return info[\"id\"]\n",
        "        else:\n",
        "            super(self.__class__, self).image_reference(image_id)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVM2xPFOvvaO"
      },
      "source": [
        "def train(model, dataset_dir):\n",
        "    \"\"\"Train the model.\"\"\"\n",
        "    # Training dataset.\n",
        "    dataset_train = BloodDataset()\n",
        "    dataset_train.load_blood(dataset_dir, \"train\")\n",
        "    dataset_train.prepare()\n",
        "\n",
        "    # Validation dataset\n",
        "    dataset_val = BloodDataset()\n",
        "    dataset_val.load_blood(dataset_dir, \"val\")\n",
        "    dataset_val.prepare()\n",
        "\n",
        "    # Image augmentation\n",
        "    # http://imgaug.readthedocs.io/en/latest/source/augmenters.html\n",
        "    augmentation = iaa.SomeOf((0, 2), [\n",
        "        iaa.Fliplr(0.5),\n",
        "        iaa.Flipud(0.5),\n",
        "        iaa.OneOf([iaa.Affine(rotate=90),\n",
        "                   iaa.Affine(rotate=180),\n",
        "                   iaa.Affine(rotate=270)]),\n",
        "        iaa.Multiply((0.8, 1.5)),\n",
        "        iaa.GaussianBlur(sigma=(0.0, 5.0))\n",
        "    ])\n",
        "\n",
        "\n",
        "    # If starting from imagenet, train heads only for a bit\n",
        "    # since they have random weights\n",
        "    print(\"Train network heads\")\n",
        "    model.train(dataset_train, dataset_val,\n",
        "                learning_rate=config.LEARNING_RATE,\n",
        "                epochs=10,\n",
        "                augmentation=augmentation,\n",
        "                layers='heads')\n",
        "\n",
        "    print(\"Train all layers\")\n",
        "    model.train(dataset_train, dataset_val,\n",
        "                learning_rate=config.LEARNING_RATE,\n",
        "                epochs=10,\n",
        "                augmentation=augmentation,\n",
        "                layers='all')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzsgYE76wiqH"
      },
      "source": [
        "config = BloodConfig()\n",
        "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
        "                                  model_dir=MODEL_DIR)\n",
        "# Start from ImageNet trained weights\n",
        "weights_path = model.get_imagenet_weights()\n",
        "model.load_weights(weights_path, by_name=True)\n",
        "\n",
        "train(model, \"/content/dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKpvLe5pwEQx"
      },
      "source": [
        " def get_ax(rows=1, cols=1, size=8):\n",
        "    \"\"\"Return a Matplotlib Axes array to be used in\n",
        "    all visualizations in the notebook. Provide a\n",
        "    central point to control graph sizes.\n",
        "    \n",
        "    Change the default size attribute to control the size\n",
        "    of rendered images\n",
        "    \"\"\"\n",
        "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6iGb6hNxWPn"
      },
      "source": [
        "inference_config = BloodInferenceConfig()\n",
        "model = modellib.MaskRCNN(mode=\"inference\", config=inference_config,\n",
        "                                  model_dir=MODEL_DIR)\n",
        "weights_path = model.find_last()\n",
        "model.load_weights(weights_path, by_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1F-JCyhwTy_"
      },
      "source": [
        "# Test on a random image\n",
        "dataset_val = BloodDataset()\n",
        "dataset_val.load_blood(\"/content/dataset\", \"val\")\n",
        "dataset_val.prepare()\n",
        "image_id = random.choice(dataset_val.image_ids)\n",
        "\n",
        "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
        "    modellib.load_image_gt(dataset_val, inference_config, \n",
        "                          image_id, use_mini_mask=False)\n",
        "\n",
        "log(\"original_image\", original_image)\n",
        "log(\"image_meta\", image_meta)\n",
        "log(\"gt_class_id\", gt_class_id)\n",
        "log(\"gt_bbox\", gt_bbox)\n",
        "log(\"gt_mask\", gt_mask)\n",
        "\n",
        "results = model.detect([original_image], verbose=1)\n",
        "\n",
        "r = results[0]\n",
        "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
        "                            dataset_val.class_names, r['scores'], ax=get_ax())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHsz1qtehbb8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}